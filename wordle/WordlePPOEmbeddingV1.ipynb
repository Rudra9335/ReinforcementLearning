{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540130a-7cd7-4463-8084-97d4edb36840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/rl/ppo_cartpole/\n",
    "#This model tries to generate an embedding based learning such that we can have general learning of agent\n",
    "#that looks into the properties of the word and takes the best action\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Multiply\n",
    "import scipy.signal\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239bbbf3-d917-4ad9-8e58-a5d7e8f5f2d6",
   "metadata": {},
   "source": [
    "### Environment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321439f0-26a9-48af-b54e-9938c419f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/cfreshman/a03ef2cba789d8cf00c08f767e0fad7b\n",
    "my_file = open(\"C:/Users/rudra/OneDrive/Desktop/reinforcementLearning/Wordle/data/wordle-answers-alphabetical.txt\", \"r\")\n",
    "content = my_file.read()\n",
    "content = list(content.split('\\n'))\n",
    "# limiting to only a 100 words to ensure that the model is converging\n",
    "content = random.sample(content, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119694c-d091-4dd5-8d42-e0157df2ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_alphabet = list(string.ascii_letters)[:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9cd5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 26*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba61d1-3aa4-472e-a493-23e8496a105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_content_embedding(content):\n",
    "    res = []\n",
    "    for word in content:\n",
    "        embedding = [0.0 for i in range(embedding_size)]\n",
    "        for index_,char_ in enumerate(word):\n",
    "            alphabet_index = lower_alphabet.index(char_)\n",
    "            embedding[26*index_ + alphabet_index] = 1.0\n",
    "        res.append(np.array(embedding))\n",
    "    return np.array(res, dtype=np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246904da-a695-4e3a-93db-8a6dfb76834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_embeddings = generate_content_embedding(content)\n",
    "bias_weights = np.zeros(content_embeddings[0].shape, dtype=np.float32)\n",
    "proxy_action_weights = [content_embeddings, bias_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6204478-2c20-4c8e-b9ae-e6ca2602cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "observation_dimensions = 182\n",
    "num_actions = embedding_size\n",
    "proxy_action_size = len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117eaf84-8ede-44f3-af37-c2ef1930d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret_word():\n",
    "    return random.choice(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29624c1b-b8f8-4951-b05f-eb899c7d56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_available_action_space():\n",
    "    return [1.0 for i in range(len(content))]\n",
    "\n",
    "def reset_guessed_alphabet_state():\n",
    "    return [0 for i in range(len(lower_alphabet))]\n",
    "\n",
    "# array of 26 which represents which alphabet is available in word\n",
    "def reset_contains_alphabet_state():\n",
    "    return [0 for i in range(len(lower_alphabet))]\n",
    "\n",
    "# Array of 26*5. \n",
    "# First 26 represent which alphabet was correctly guessed at the first slot\n",
    "# Second 26 represent which alphabet was correctly guessed at the second slot. And so on for the next 5 slots.\n",
    "def reset_correct_alphabet_pos_state():\n",
    "    return [0 for i in range(len(lower_alphabet)*5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86be32c-4281-48a0-b0a1-3cb2028ade5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_AVAILABLE_ACTION_SPACE(action):\n",
    "    AVAILABLE_ACTION_SPACE[action] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52657b9-e7be-4691-9d0f-3cfaffb9ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset():\n",
    "    AVAILABLE_ACTION_SPACE = reset_available_action_space()\n",
    "    guessed_alphabet_state = reset_guessed_alphabet_state()\n",
    "    contains_alphabet_state = reset_contains_alphabet_state()\n",
    "    correct_alphabet_pos_state = reset_correct_alphabet_pos_state()\n",
    "    state = np.array(guessed_alphabet_state + contains_alphabet_state + correct_alphabet_pos_state)\n",
    "    SECRET_WORD = get_secret_word()\n",
    "    return state, SECRET_WORD, AVAILABLE_ACTION_SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95568382-199c-495b-a2e5-12c67583ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action, observation):\n",
    "    state = observation[0].tolist()\n",
    "    guessed_word = content[action]\n",
    "    guessed_alphabet_state = state[:26]\n",
    "    contains_alphabet_state = state[26:52]\n",
    "    correct_alphabet_pos_state = state[52:]\n",
    "    \n",
    "    done = False\n",
    "    reward = 0\n",
    "    \n",
    "    if SECRET_WORD == guessed_word:\n",
    "        done = True\n",
    "        reward = 10\n",
    "    secret_word = list(SECRET_WORD)\n",
    "    guessed_word = list(guessed_word)\n",
    "    for index_, char_ in enumerate(guessed_word):\n",
    "        alphabet_index = lower_alphabet.index(char_)\n",
    "        guessed_alphabet_state[alphabet_index] = 1\n",
    "        if char_ in secret_word:\n",
    "            contains_alphabet_state[alphabet_index] = 1\n",
    "            if secret_word[index_] == char_:\n",
    "                correct_alphabet_pos_state[26*index_ + alphabet_index] = 1\n",
    "    state = np.array(guessed_alphabet_state + contains_alphabet_state + correct_alphabet_pos_state)\n",
    "    return state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ed43a-3efc-4421-9cd1-139f4d69d794",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57e28c-bcae-4afb-9f3c-93270dfd5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "def add_proxy_action_layer(x, proxy_action_size, proxy_action_weights):\n",
    "    return layers.Dense(units = proxy_action_size)(x)\n",
    "\n",
    "    \n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, proxy_action_size) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "# @tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    available_action_tensor = tf.expand_dims(tf.convert_to_tensor(AVAILABLE_ACTION_SPACE), 0)\n",
    "    logits = tf.add(logits, 1.0)\n",
    "    logits = Multiply()([logits, available_action_tensor])\n",
    "    logits = tf.add(logits, -1.0)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43419b61-21dd-4c9b-a958-1eb1458dc861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "games_per_epoch = 100\n",
    "game_length = 6\n",
    "epochs = 10000\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 3e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 80\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.01\n",
    "hidden_sizes = (256, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31e00-f313-4d3c-af89-655ca463ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, games_per_epoch*game_length)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, tf.tanh)\n",
    "logits = add_proxy_action_layer(logits, proxy_action_size, proxy_action_weights)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "actor.layers[-1].set_weights(proxy_action_weights)\n",
    "actor.layers[-1].trainable = False\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, SECRET_WORD, AVAILABLE_ACTION_SPACE = env_reset()\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241356da-92e2-4136-8adb-15bc59e271aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    # Iterate over the steps of each epoch\n",
    "    for game_in_epoch in range(games_per_epoch):\n",
    "        for game in range(game_length):\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "            observation = observation.reshape(1, -1)\n",
    "            logits, action = sample_action(observation)\n",
    "            action_index = int(action[0].numpy())\n",
    "            update_AVAILABLE_ACTION_SPACE(action_index)\n",
    "            observation_new, reward, done = env_step(action_index, observation)\n",
    "            reward = reward * (game_length - game)\n",
    "            episode_return += reward\n",
    "            episode_length += 1\n",
    "\n",
    "            # Get the value and log-probability of the action\n",
    "            value_t = critic(observation)\n",
    "            logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "            # Store obs, act, rew, v_t, logp_pi_t\n",
    "            buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "            # Update the observation\n",
    "            observation = observation_new\n",
    "\n",
    "            # Finish trajectory if reached to a terminal state\n",
    "            terminal = done\n",
    "            if terminal or game == game_length-1:\n",
    "                last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "                buffer.finish_trajectory(last_value)\n",
    "                sum_return += episode_return\n",
    "                sum_length += episode_length\n",
    "                num_episodes += 1\n",
    "                observation, SECRET_WORD, AVAILABLE_ACTION_SPACE = env_reset()\n",
    "                episode_return, episode_length = 0, 0\n",
    "                break\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "        )       \n",
    "# Model converged to about 2.8 average attempts for every 100 games "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46b9fa-e6b4-49eb-b9c2-fe413a341105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
